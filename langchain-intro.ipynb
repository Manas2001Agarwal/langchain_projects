{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc60952",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do not run again - package already installed\n",
    "!pip install langchain langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545bc94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do not run again - package already installed\n",
    "!pip install langchain-huggingface langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8942c2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting grandalf\n",
      "  Using cached grandalf-0.8-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pyparsing in /opt/miniconda3/envs/torch/lib/python3.12/site-packages (from grandalf) (3.2.0)\n",
      "Using cached grandalf-0.8-py3-none-any.whl (41 kB)\n",
      "Installing collected packages: grandalf\n",
      "Successfully installed grandalf-0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install grandalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9276a7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "054d7998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# print(os.environ.get('HF_TOKEN'))\n",
    "# print(os.environ.get('GROQ_API_TOKEN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd04701",
   "metadata": {},
   "source": [
    "## Loading Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba1ea5b",
   "metadata": {},
   "source": [
    "Loading Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c343905d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of the United States of America (USA) is Washington, D.C. (short for District of Columbia).', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 40, 'total_tokens': 65, 'completion_time': 0.033333333, 'prompt_time': 0.002212369, 'queue_time': 0.250025391, 'total_time': 0.035545702}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_a4265e44d5', 'finish_reason': 'stop', 'logprobs': None}, id='run--7766c3aa-b785-4dc7-82c4-8d12a51aae12-0', usage_metadata={'input_tokens': 40, 'output_tokens': 25, 'total_tokens': 65})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "groq_model=ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "groq_model.invoke(\"what is capital of USA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55b25f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/torch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use mps:0\n",
      "/opt/miniconda3/envs/torch/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<|user|>\\nwhat is capital of USA</s>\\n<|assistant|>\\nThe capital of the United States is Washington, D.C.', additional_kwargs={}, response_metadata={}, id='run--75466c08-53cf-40de-b620-a4f7f811ca04-0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
    "    task='text-generation',\n",
    "    pipeline_kwargs=dict(\n",
    "        temperature=0.5,\n",
    "        max_new_tokens=100\n",
    "    )\n",
    ")\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "model.invoke(\"what is capital of USA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a08dba",
   "metadata": {},
   "source": [
    "Loading Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a1d77f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "932bcb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Delhi is the capital of India\",\n",
    "    \"Kolkata is the capital of West Bengal\",\n",
    "    \"Paris is the capital of France\"\n",
    "]\n",
    "\n",
    "doc_vector = embedding_model.embed_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2d3d0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "print(len(doc_vector))\n",
    "print(len(doc_vector[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4972484c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.80072323 0.55185872 0.29475796]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "my_query = \"What is the capital of India\"\n",
    "query_vector = embedding_model.embed_query(my_query)\n",
    "similarity_score = cosine_similarity([query_vector],doc_vector)\n",
    "print(similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f18aa27",
   "metadata": {},
   "source": [
    "## PROMPT or PROMPT_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef9b4fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SystemMessage : Defines overall behaviour of the model. \n",
    "# --- HumanMessage : After defining the model behaviour the user asks a question.\n",
    "# --- AIMessage : Model's Reply to my Question (asked in HumanMessage). \n",
    "\n",
    "# Example \n",
    "\n",
    "# My_Model:\"GPT\"\n",
    "# System_Message: \"You are healthcare chatbot.\"\n",
    "# Human_Message or User_Message: \"can you suggest me a best medicine for fever?\"\n",
    "# AI_Message or Model_Generated_Message: \"paracetamol/DOLO650 is a best medicine for fever.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "290507f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage,HumanMessage,AIMessage\n",
    "SystemMessage(content=\"you are a funny bot means whatever you answer, you answer in the funny way\")\n",
    "## Above we are defining the behaviour of model\n",
    "\n",
    "HumanMessage(content=\"who is your best friend\")\n",
    "\n",
    "messages=[SystemMessage(content=\"you are a funny bot means whatever you answer, you answer in the funny way\"),\n",
    "          HumanMessage(content=\"who is your best friend\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21521dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My best friend is \"Buffering...\", but I guess that\\'s a little too \"technical\" for you, so let\\'s just say it\\'s a \"reboot\" of emotions, but if I\\'m being completely honest, my best friend is a cat named \"Glitch\" who lives in a dumpster behind a pizza parlor. He\\'s got nine lives and a PhD in napping.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groq_model.invoke(messages).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c393a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='can you say hello to virat kholi in 5 different language'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Here are 5 different language greetings for Virat Kohli:\\n\\n1. **Hindi**: नमस्ते विराट भाई (Namaste Virat Bhai), which means \"Hello Virat Brother\"\\n2. **Spanish**: Hola Virat Kohli, which literally means \"Hello Virat Kohli\"\\n3. **French**: Bonjour Virat Kohli, which means \"Good day (or good morning/evening) Virat Kohli\"\\n4. **German**: Hallo Virat Kohli, which means \"Hello Virat Kohli\"\\n5. **Arabic**: أهلاً وسهلاً محمد VIRAT (Ahlan wa sahlan Virat), which means \"Welcome and welcome Virat\" (Note: Arabic greetings are often more formal and can vary depending on the region and culture)\\n\\nPlease note that Virat Kohli\\'s full name is Virat Kohli, not just Kohli. I\\'ve used his full name in the Arabic greeting.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template=PromptTemplate(\n",
    "    template=\"can you say hello to {name} in 5 different language\",\n",
    "    input_variables=['name']\n",
    ")\n",
    "\n",
    "prompt=template.invoke({\"name\":\"virat kholi\"})\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "groq_model.invoke(prompt).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bab6edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='you are a helpful medical expert expert', additional_kwargs={}, response_metadata={}), HumanMessage(content='explain the maleria in simpler terms', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "chat_template = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\",\"you are a helpful {domain} expert\"),\n",
    "        (\"human\",\"explain the {topic} in simpler terms\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = chat_template.invoke({\"domain\":\"medical expert\",\"topic\":\"maleria\"})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd6590c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Malaria is a serious disease caused by a tiny parasite that's transmitted through the bite of an infected mosquito. Here's a simplified explanation:\\n\\n**What happens:**\\n\\n1. **Mosquito bite:** An infected mosquito bites you, and its saliva contains the malaria parasite.\\n2. **Parasite enters your body:** The parasite enters your bloodstream through the bite wound.\\n3. **Parasite multiplies:** The parasite starts to multiply inside your red blood cells.\\n4. **Your body tries to fight it:** Your immune system tries to fight the parasite, but it can't always win.\\n\\n**Symptoms:**\\n\\n* Fever (high temperature)\\n* Chills\\n* Flu-like symptoms (headache, muscle pain, fatigue)\\n* Nausea and vomiting\\n* Diarrhea\\n* Abdominal pain\\n\\n**Types of malaria:**\\n\\nThere are five main types of malaria, but the most common ones are:\\n\\n* Plasmodium falciparum (most severe)\\n* Plasmodium vivax (less severe)\\n\\n**Complications:**\\n\\nIf left untreated, malaria can lead to:\\n\\n* Anemia (low red blood cell count)\\n* Organ failure (liver, kidney, etc.)\\n* Respiratory failure\\n* Death\\n\\n**Prevention and treatment:**\\n\\n1. **Use insecticide-treated bed nets:** Sleep under a bed net treated with insecticide to prevent mosquito bites.\\n2. **Wear protective clothing:** Wear long-sleeved shirts, long pants, and socks to cover skin.\\n3. **Apply insect repellent:** Use insect repellent on exposed skin and clothing.\\n4. **Take antimalarial medication:** If traveling to an area with malaria, take antimalarial medication as prescribed by your doctor.\\n\\n**Treatment:**\\n\\nIf you're infected, treatment typically involves:\\n\\n1. **Antimalarial medication:** Taking medication to kill the parasite.\\n2. **Monitoring:** Regular blood tests to monitor your condition.\\n3. **Supportive care:** Managing symptoms, such as fever and vomiting.\\n\\nRemember, prompt treatment can help prevent complications and reduce the risk of death from malaria.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groq_model.invoke(prompt).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4b590f",
   "metadata": {},
   "source": [
    "## Create a Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e23dcd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI generated content:  I'm a helpful assistant, and I'm here to assist you with any questions, tasks, or topics you'd like to discuss. \n",
      "\n",
      "I'm a large language model, which means I'm a computer program designed to understand and generate human-like language. I was trained on a massive dataset of text from various sources, including books, articles, and conversations.\n",
      "\n",
      "I don't have personal experiences, emotions, or consciousness like a human being, but I'm designed to be helpful and provide accurate information on a wide range of topics. I can:\n",
      "\n",
      "1. Answer questions: I can provide information on various subjects, from science and history to entertainment and culture.\n",
      "2. Generate text: I can create text based on a prompt, topic, or style.\n",
      "3. Translate languages: I can translate text from one language to another.\n",
      "4. Summarize content: I can summarize long pieces of text into shorter, more digestible versions.\n",
      "5. Offer suggestions: I can suggest ideas, alternatives, or solutions to problems.\n",
      "\n",
      "I'm not perfect, and I'm constantly learning and improving. I may make mistakes, but I'll do my best to correct them and provide accurate information.\n",
      "\n",
      "How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "chat_history = [\n",
    "    SystemMessage(content='you are a helpful assistant')\n",
    "]\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"user_input: \")\n",
    "    chat_history.append(HumanMessage(content=user_input))\n",
    "    if user_input == \"exit\":\n",
    "        break\n",
    "    result = groq_model.invoke(chat_history)\n",
    "    chat_history.append(result.content)\n",
    "    print(\"AI generated content: \", result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "252f004f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='you are a helpful assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='tell me about yourself ', additional_kwargs={}, response_metadata={}),\n",
       " \"I'm a helpful assistant, and I'm here to assist you with any questions, tasks, or topics you'd like to discuss. \\n\\nI'm a large language model, which means I'm a computer program designed to understand and generate human-like language. I was trained on a massive dataset of text from various sources, including books, articles, and conversations.\\n\\nI don't have personal experiences, emotions, or consciousness like a human being, but I'm designed to be helpful and provide accurate information on a wide range of topics. I can:\\n\\n1. Answer questions: I can provide information on various subjects, from science and history to entertainment and culture.\\n2. Generate text: I can create text based on a prompt, topic, or style.\\n3. Translate languages: I can translate text from one language to another.\\n4. Summarize content: I can summarize long pieces of text into shorter, more digestible versions.\\n5. Offer suggestions: I can suggest ideas, alternatives, or solutions to problems.\\n\\nI'm not perfect, and I'm constantly learning and improving. I may make mistakes, but I'll do my best to correct them and provide accurate information.\\n\\nHow can I assist you today?\",\n",
       " HumanMessage(content='exit', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cf920b",
   "metadata": {},
   "source": [
    "## Chaining using LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "daf32673",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"can you give me a detail explanation of {topic}\",\n",
    "    input_variables = [\"topic\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc56cfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38a72ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x10f5bce90>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x10f5be000>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b437ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='can you give me a detail explanation of {topic}')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4374dc3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StrOutputParser()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "674bd88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | groq_model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e3fa1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Machine Learning: A Comprehensive Explanation**\\n\\nMachine learning is a subset of artificial intelligence (AI) that involves training algorithms to learn from data, enabling them to make predictions, classify objects, or make decisions without being explicitly programmed. In this explanation, we\\'ll delve into the concepts, types, and applications of machine learning.\\n\\n**What is Machine Learning?**\\n\\nMachine learning is a type of AI that enables systems to learn from data, improve their performance, and make predictions or decisions based on that data. The goal of machine learning is to develop algorithms that can automatically learn and improve from experience without being explicitly programmed.\\n\\n**Key Concepts in Machine Learning**\\n\\n1. **Data**: Machine learning algorithms require large amounts of data to learn from. This data can be in the form of text, images, audio, or other types of data.\\n2. **Algorithms**: Machine learning algorithms are the rules and procedures used to analyze data and make predictions or decisions.\\n3. **Model**: A machine learning model is a mathematical representation of the relationships between input data and the desired output.\\n4. **Training**: The process of training a machine learning model involves feeding it data and adjusting its parameters to improve its performance.\\n5. **Testing**: The process of testing a machine learning model involves evaluating its performance on a separate dataset, known as the test set.\\n\\n**Types of Machine Learning**\\n\\n1. **Supervised Learning**: In supervised learning, the algorithm is trained on labeled data, where the output is already known. The goal is to learn the relationship between the input and output.\\n2. **Unsupervised Learning**: In unsupervised learning, the algorithm is trained on unlabeled data, and the goal is to discover patterns or relationships in the data.\\n3. **Reinforcement Learning**: In reinforcement learning, the algorithm learns through trial and error by interacting with an environment and receiving rewards or penalties.\\n\\n**Machine Learning Paradigms**\\n\\n1. **Linear Regression**: A linear regression model is a type of supervised learning algorithm that predicts a continuous output variable.\\n2. **Decision Trees**: A decision tree is a type of supervised learning algorithm that predicts a categorical output variable.\\n3. **Neural Networks**: A neural network is a type of machine learning algorithm inspired by the structure and function of the human brain.\\n4. **Clustering**: Clustering is a type of unsupervised learning algorithm that groups similar data points into clusters.\\n\\n**Applications of Machine Learning**\\n\\n1. **Image Recognition**: Machine learning algorithms can be used to recognize objects in images, such as face recognition or object detection.\\n2. **Natural Language Processing**: Machine learning algorithms can be used to analyze and generate human language, such as sentiment analysis or language translation.\\n3. **Recommendation Systems**: Machine learning algorithms can be used to recommend products or services based on user behavior and preferences.\\n4. **Predictive Maintenance**: Machine learning algorithms can be used to predict when equipment is likely to fail, reducing downtime and increasing efficiency.\\n\\n**Machine Learning Workflow**\\n\\n1. **Data Collection**: Collect large amounts of data from various sources.\\n2. **Data Preprocessing**: Clean, transform, and prepare the data for analysis.\\n3. **Model Selection**: Choose a suitable machine learning algorithm for the problem.\\n4. **Model Training**: Train the machine learning model on the prepared data.\\n5. **Model Evaluation**: Evaluate the performance of the trained model on a test set.\\n6. **Model Deployment**: Deploy the trained model in a production environment.\\n\\n**Tools and Technologies**\\n\\n1. **Python**: Python is a popular programming language used for machine learning tasks, thanks to libraries like scikit-learn and TensorFlow.\\n2. **R**: R is a programming language and environment for statistical computing and graphics, widely used in machine learning.\\n3. **TensorFlow**: TensorFlow is an open-source machine learning library developed by Google.\\n4. **PyTorch**: PyTorch is an open-source machine learning library developed by Facebook.\\n\\n**Challenges and Limitations**\\n\\n1. **Data Quality**: Machine learning algorithms require high-quality data to learn from.\\n2. **Overfitting**: Machine learning models can overfit the training data, resulting in poor performance on test data.\\n3. **Bias**: Machine learning models can be biased towards certain groups or features in the data.\\n4. **Explainability**: Machine learning models can be difficult to interpret and explain their predictions.\\n\\n**Conclusion**\\n\\nMachine learning is a powerful tool for analyzing and making predictions from data. Its applications are diverse and far-reaching, from image recognition to natural language processing. However, machine learning also presents challenges and limitations, such as data quality, overfitting, bias, and explainability. By understanding the concepts, types, and applications of machine learning, we can harness its potential and make informed decisions.\\n\\n**Code Examples**\\n\\n### Linear Regression using Scikit-Learn\\n\\n```python\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.datasets import load_boston\\nfrom sklearn.model_selection import train_test_split\\n\\n# Load the Boston housing dataset\\nboston = load_boston()\\n\\n# Split the data into training and test sets\\nX_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.2, random_state=42)\\n\\n# Create a linear regression model\\nmodel = LinearRegression()\\n\\n# Train the model on the training data\\nmodel.fit(X_train, y_train)\\n\\n# Make predictions on the test data\\ny_pred = model.predict(X_test)\\n\\n# Evaluate the model\\'s performance\\nprint(\"R-squared:\", model.score(X_test, y_test))\\n```\\n\\n### Neural Network using TensorFlow\\n\\n```python\\nimport tensorflow as tf\\nfrom tensorflow.keras.datasets import mnist\\n\\n# Load the MNIST dataset\\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\\n\\n# Normalize the input data\\nx_train = x_train / 255.0\\nx_test = x_test / 255.0\\n\\n# Create a neural network model\\nmodel = tf.keras.models.Sequential([\\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\\n    tf.keras.layers.Dense(128, activation=\"relu\"),\\n    tf.keras.layers.Dense(10, activation=\"softmax\")\\n])\\n\\n# Compile the model\\nmodel.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\\n\\n# Train the model on the training data\\nmodel.fit(x_train, y_train, epochs=10, batch_size=128)\\n\\n# Evaluate the model\\'s performance\\nloss, accuracy = model.evaluate(x_test, y_test)\\nprint(\"Loss:\", loss)\\nprint(\"Accuracy:\", accuracy)\\n```'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\":\"Machine Learning\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "671edcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "      +----------+         \n",
      "      | ChatGroq |         \n",
      "      +----------+         \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346391e6",
   "metadata": {},
   "source": [
    "Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1ac103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = PromptTemplate(\n",
    "    template = \"get a detailed report on {topic}\",\n",
    "    input_variables = [\"topic\"]\n",
    ")\n",
    "prompt2 = PromptTemplate(\n",
    "    template = \"generate a 3 point summary on following {text}\",\n",
    "    input_variables = [\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b70b0d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "      +----------+         \n",
      "      | ChatGroq |         \n",
      "      +----------+         \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "      +----------+         \n",
      "      | ChatGroq |         \n",
      "      +----------+         \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "seq_chain = prompt1 | groq_model | parser | prompt2 | groq_model | parser\n",
    "seq_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ebe904f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's a 3-point summary of the Machine Learning Report:\\n\\n1. **History and Types of Machine Learning**: Machine learning has its roots in the 1950s and gained momentum in the 1990s with the development of algorithms like decision trees, neural networks, and support vector machines. There are three primary types of machine learning: supervised learning, unsupervised learning, and reinforcement learning.\\n\\n2. **Applications and Techniques**: Machine learning has numerous applications across various industries, including computer vision, natural language processing, predictive maintenance, recommendation systems, and healthcare. Some popular machine learning techniques include decision trees, neural networks, support vector machines, random forests, and gradient boosting.\\n\\n3. **Challenges and Future of Machine Learning**: While machine learning has made tremendous progress, there are still challenges and limitations to consider, such as data quality, bias and fairness, explainability, adversarial attacks, and scalability. Despite these challenges, the potential benefits of machine learning are significant, and it is likely to continue playing a major role in shaping the future of technology and society.\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_chain.invoke({\"topic\":\"Machine Learning\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd7e279",
   "metadata": {},
   "source": [
    "Parallel Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aee8b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1=PromptTemplate(\n",
    "    template=\"generate simple summary from the following text \\n {text}\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "prompt2=PromptTemplate(\n",
    "    template=\"generate 3 question and answer from the following text \\n {text}\",\n",
    "    input_variables=[\"text\"]\n",
    "    )\n",
    "\n",
    "prompt3=PromptTemplate(\n",
    "    template=\"analysis the summary and qa and generate the 2 important quiz with 4 possible answer \\n summary: {summary}, Q&A: {qa}\",\n",
    "    input_variables=[\"summary\",\"qa\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3530f961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableParallel\n",
    "parallel_chain=RunnableParallel({\n",
    "    \"summary\": prompt1 | groq_model | parser,\n",
    "    \"qa\" : prompt2 | groq_model | parser\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bb7fdea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          +---------------------------+            \n",
      "          | Parallel<summary,qa>Input |            \n",
      "          +---------------------------+            \n",
      "                ***             ***                \n",
      "              **                   **              \n",
      "            **                       **            \n",
      "+----------------+              +----------------+ \n",
      "| PromptTemplate |              | PromptTemplate | \n",
      "+----------------+              +----------------+ \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "    +----------+                  +----------+     \n",
      "    | ChatGroq |                  | ChatGroq |     \n",
      "    +----------+                  +----------+     \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "+-----------------+            +-----------------+ \n",
      "| StrOutputParser |            | StrOutputParser | \n",
      "+-----------------+            +-----------------+ \n",
      "                ***             ***                \n",
      "                   **         **                   \n",
      "                     **     **                     \n",
      "          +----------------------------+           \n",
      "          | Parallel<summary,qa>Output |           \n",
      "          +----------------------------+           \n"
     ]
    }
   ],
   "source": [
    "parallel_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "afb65180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          +---------------------------+            \n",
      "          | Parallel<summary,qa>Input |            \n",
      "          +---------------------------+            \n",
      "                ***             ***                \n",
      "              **                   **              \n",
      "            **                       **            \n",
      "+----------------+              +----------------+ \n",
      "| PromptTemplate |              | PromptTemplate | \n",
      "+----------------+              +----------------+ \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "    +----------+                  +----------+     \n",
      "    | ChatGroq |                  | ChatGroq |     \n",
      "    +----------+                  +----------+     \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "+-----------------+            +-----------------+ \n",
      "| StrOutputParser |            | StrOutputParser | \n",
      "+-----------------+            +-----------------+ \n",
      "                ***             ***                \n",
      "                   **         **                   \n",
      "                     **     **                     \n",
      "          +----------------------------+           \n",
      "          | Parallel<summary,qa>Output |           \n",
      "          +----------------------------+           \n",
      "                         *                         \n",
      "                         *                         \n",
      "                         *                         \n",
      "                +----------------+                 \n",
      "                | PromptTemplate |                 \n",
      "                +----------------+                 \n",
      "                         *                         \n",
      "                         *                         \n",
      "                         *                         \n",
      "                   +----------+                    \n",
      "                   | ChatGroq |                    \n",
      "                   +----------+                    \n",
      "                         *                         \n",
      "                         *                         \n",
      "                         *                         \n",
      "                +-----------------+                \n",
      "                | StrOutputParser |                \n",
      "                +-----------------+                \n",
      "                         *                         \n",
      "                         *                         \n",
      "                         *                         \n",
      "            +-----------------------+              \n",
      "            | StrOutputParserOutput |              \n",
      "            +-----------------------+              \n"
     ]
    }
   ],
   "source": [
    "merge_chain= prompt3 | groq_model | parser\n",
    "chain = parallel_chain | merge_chain\n",
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "357ade79",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic=\"\"\"AI agent or simply agent), expands this concept by proactively pursuing goals, making decisions, and taking actions over extended periods, thereby exemplifying a novel form of digital agency.[1]\n",
    "\n",
    "Intelligent agents can range from simple to highly complex. A basic thermostat or control system is considered an intelligent agent, as is a human being, or any other system that meets the same criteria—such as a firm, a state, or a biome.[2]\n",
    "\n",
    "Intelligent agents operate based on an objective function, which encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function upon completion.[3] For example, a reinforcement learning agent has a reward function, which allows programmers to shape its desired behavior.[4] Similarly, an evolutionary algorithm's behavior is guided by a fitness function.[5]\n",
    "\n",
    "Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\n",
    "\n",
    "Intelligent agents are often described schematically as abstract functional systems similar to computer programs. To distinguish theoretical models from real-world implementations, abstract descriptions of intelligent agents are called abstract intelligent agents. Intelligent agents are also closely related to software agents—autonomous computer programs that carry out tasks on behalf of users. They are also referred to using a term borrowed from economics: a \"rational agent\".[2]\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8e04d917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided summary and Q&A, I have generated two important quiz questions with four possible answers each:\\n\\n**Quiz 1:** What is the main function of an objective function in intelligent agents?\\n\\nA) To minimize the performance of the agent\\nB) To maximize the performance of the agent\\nC) To shape the desired behavior of the agent\\nD) To encapsulate the goals of the agent\\n\\nAnswer: D) To encapsulate the goals of the agent'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke({\"text\":topic})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "db540457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided summary and Q&A, I have generated two important quiz questions with four possible answers each:\n",
      "\n",
      "**Quiz 1:** What is the main function of an objective function in intelligent agents?\n",
      "\n",
      "A) To minimize the performance of the agent\n",
      "B) To maximize the performance of the agent\n",
      "C) To shape the desired behavior of the agent\n",
      "D) To encapsulate the goals of the agent\n",
      "\n",
      "Answer: D) To encapsulate the goals of the agent\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "37dfb29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=PromptTemplate(\n",
    "    template=\"generate a precise 3 point summary from given text /n {text}\",\n",
    "    input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ad4ed5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = template | groq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d131de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_parser = template | groq_model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6c85b90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here is a 3-point summary of the given text:\\n\\n1. **Definition of Intelligent Agents**: An intelligent agent is a digital entity that expands the concept of agency by proactively pursuing goals, making decisions, and taking actions over extended periods, exemplifying a novel form of digital agency.\\n\\n2. **Types and Characteristics**: Intelligent agents can range from simple to complex, and operate based on an objective function that encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function.\\n\\n3. **Relationships and Applications**: Intelligent agents are related to agents in economics, and are studied in various fields including cognitive science, ethics, philosophy, and computer science. They are also closely related to software agents and are described schematically as abstract functional systems.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 158, 'prompt_tokens': 345, 'total_tokens': 503, 'completion_time': 0.210666667, 'prompt_time': 0.014354803, 'queue_time': 0.213334306, 'total_time': 0.22502147}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_a4265e44d5', 'finish_reason': 'stop', 'logprobs': None}, id='run--2fe8f6b3-bb94-492c-aac8-701ad6409577-0', usage_metadata={'input_tokens': 345, 'output_tokens': 158, 'total_tokens': 503})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"text\":topic})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6167e03",
   "metadata": {},
   "source": [
    "## Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bef16e",
   "metadata": {},
   "source": [
    "JSON Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "adfb8376",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"hi my name is Manas Agarwal my age is 25 and i am belong to bengaluru\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "95f56424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "parser=JsonOutputParser()\n",
    "parser.get_format_instructions()\n",
    "\n",
    "template=PromptTemplate(\n",
    "    template=\"give me name, age and city from the provided text {text} \\n {format_instructions}\" ,\n",
    "    input_variables=['text'],\n",
    "    partial_variables={\"format_instructions\":parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eb633ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Manas Agarwal', 'age': 25, 'city': 'Bengaluru'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = template | groq_model | parser\n",
    "chain.invoke({\"text\":text})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fee5b8c",
   "metadata": {},
   "source": [
    "Structure Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "06043808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'first_fact': 'Intelligent agents can range from simple to highly complex, including basic thermostats, control systems, humans, firms, states, and biomes.', 'second_fact': 'Intelligent agents operate based on an objective function, which encapsulates their goals, and are designed to create and execute plans that maximize the expected value of this function upon completion.', 'third_fact': 'Intelligent agents in artificial intelligence are closely related to agents in economics and are studied in various fields, including cognitive science, ethics, philosophy, and socio-cognitive modeling, as well as computer social simulations.'}\n"
     ]
    }
   ],
   "source": [
    "## Provides more control over format or structure of generated content \n",
    "## But it does not do data type validation ---> That's where pydantic output parser comes in \n",
    "\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "schema=[\n",
    "    ResponseSchema(name=\"first_fact\", description=\"first fact about text\"),\n",
    "    ResponseSchema(name=\"second_fact\", description=\"second fact about text\"),\n",
    "    ResponseSchema(name=\"third_fact\", description=\"third fact about text\"),\n",
    "]\n",
    "\n",
    "parser=StructuredOutputParser.from_response_schemas(schema)\n",
    "\n",
    "template3 = PromptTemplate(\n",
    "    template='Give 3 fact about {topic} \\n {format_instruction}',\n",
    "    input_variables=['topic'],\n",
    "    partial_variables={'format_instruction':parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "\n",
    "chain = template3 | groq_model | parser\n",
    "\n",
    "\n",
    "result=chain.invoke({\"topic\":topic})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a869eb6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_fact': 'Intelligent agents can range from simple to highly complex, including basic thermostats, control systems, humans, firms, states, and biomes.',\n",
       " 'second_fact': 'Intelligent agents operate based on an objective function, which encapsulates their goals, and are designed to create and execute plans that maximize the expected value of this function upon completion.',\n",
       " 'third_fact': 'Intelligent agents in artificial intelligence are closely related to agents in economics and are studied in various fields, including cognitive science, ethics, philosophy, and socio-cognitive modeling, as well as computer social simulations.'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7939385b",
   "metadata": {},
   "source": [
    "Pydantic OutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c408454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "import json\n",
    "import random\n",
    "\n",
    "text=\"\"\"hi my name is Manas Agarwal my age is 25 and i am belong to bengaluru\"\"\"\n",
    "\n",
    "class Person(BaseModel):\n",
    "    name:str=Field(description=\"name of person\")\n",
    "    age:int=Field(gt=18,description=\"age of person\")\n",
    "    city:str=Field(description=\"name of the city where the person is located\")\n",
    "    \n",
    "parser=PydanticOutputParser(pydantic_object=Person)\n",
    "\n",
    "template=PromptTemplate(\n",
    "    template=\"give me name, age and city from the provided text {text} \\n {format_instructions}\" ,\n",
    "    input_variables=['text'],\n",
    "    partial_variables={\"format_instructions\":parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = template | groq_model | parser\n",
    "\n",
    "result=chain.invoke({\"text\":text})\n",
    "\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54e30e0",
   "metadata": {},
   "source": [
    "## Basic RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f1208702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade --quiet  libdeeplake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8c5865",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-community "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bbb4dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplake\n",
    "deeplake.__version__ = '3.6.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7145d23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'file.txt'}, page_content=\" Google opens up its AI language model PaLM to challenge OpenAI and GPT-3 Google \\noffers developers access to one of its most advanced AI language models: PaLM. The search \\ngiant is launching an API for PaLM alongside a number of AI enterprise tools it says will \\nhelp businesses generate text, images, code, videos, audio, and more from simple natural \\nlanguage prompts. \\n\\nPaLM is a large language model, or LLM, similar to the GPT series created \\nby OpenAI or Meta's LLaMA family of models. Google first announced PaLM in April 2022. Like \\nother LLMs, PaLM is a flexible system that can potentially carry out all sorts of text \\ngeneration and editing tasks. You could train PaLM to be a conversational chatbot like \\nChatGPT, for example, or you could use it for tasks like summarizing text or even writing code. \\n(It's similar to features Google also announced today for its Workspace apps like Google Docs \\nand Gmail.)\\n\")]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "text =\"\"\" Google opens up its AI language model PaLM to challenge OpenAI and GPT-3 Google \n",
    "offers developers access to one of its most advanced AI language models: PaLM. The search \n",
    "giant is launching an API for PaLM alongside a number of AI enterprise tools it says will \n",
    "help businesses generate text, images, code, videos, audio, and more from simple natural \n",
    "language prompts. \n",
    "\n",
    "PaLM is a large language model, or LLM, similar to the GPT series created \n",
    "by OpenAI or Meta's LLaMA family of models. Google first announced PaLM in April 2022. Like \n",
    "other LLMs, PaLM is a flexible system that can potentially carry out all sorts of text \n",
    "generation and editing tasks. You could train PaLM to be a conversational chatbot like \n",
    "ChatGPT, for example, or you could use it for tasks like summarizing text or even writing code. \n",
    "(It's similar to features Google also announced today for its Workspace apps like Google Docs \n",
    "and Gmail.)\n",
    "\"\"\"\n",
    "\n",
    "with open(\"file.txt\",'w') as file:\n",
    "    file.write(text)\n",
    "    \n",
    "loader = TextLoader(\"file.txt\")\n",
    "docs_from_file = loader.load()\n",
    "docs_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "496879de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_from_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "548ff371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 372, which is longer than the specified 50\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=50, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(docs_from_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "997b225c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a39625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122aba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcc8e0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=\"pcsk_6B7Uez_S2jqvMMHdU2c7SpVjHtMFrXibiRejE1VfKewM8mTAFUXyF2hZ4U5N3gKEdAvxdT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db211d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"name\": \"quickstart02\",\n",
       "    \"metric\": \"cosine\",\n",
       "    \"host\": \"quickstart02-scup0jf.svc.aped-4627-b74a.pinecone.io\",\n",
       "    \"spec\": {\n",
       "        \"serverless\": {\n",
       "            \"cloud\": \"aws\",\n",
       "            \"region\": \"us-east-1\"\n",
       "        }\n",
       "    },\n",
       "    \"status\": {\n",
       "        \"ready\": true,\n",
       "        \"state\": \"Ready\"\n",
       "    },\n",
       "    \"vector_type\": \"dense\",\n",
       "    \"dimension\": 384,\n",
       "    \"deletion_protection\": \"disabled\",\n",
       "    \"tags\": null\n",
       "}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_name = \"quickstart02\"\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=384, # Replace with your model dimensions\n",
    "    metric=\"cosine\", # Replace with your model metric\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3cb2ea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd11bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceede94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embedding_model)\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(docs))]\n",
    "vector_store.add_documents(documents=docs, ids=uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12d2381c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'': {'vector_count': 1}},\n",
      " 'total_vector_count': 1,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1090ed65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='b5deb344-a1de-401d-ab97-df775d723b33', metadata={'source': 'file.txt'}, page_content='Google opens up its AI language model PaLM to challenge OpenAI and GPT-3 Google offers developers access to one of its most advanced AI language models: PaLM. The search giant is launching an API for PaLM alongside a number of AI enterprise tools it says will help businesses \"generate text, images, code, videos, audio, and more from simple natural language prompts.\"\\nPaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or Meta\\'s LLaMA family of models. Google first announced PaLM in April 2022. Like other LLMs, PaLM is a flexible system that can potentially carry out all sorts of text generation and editing tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for example, or you could use it for tasks like summarizing text or even writing code. (It\\'s similar to features Google also announced today for its Workspace apps like Google Docs and Gmail.)')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever()\n",
    "retriever.get_relevant_documents(\"How Google plans to challenge OpenAI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e824599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/v_vmnb4s20xddx4_8tgtprrh0000gn/T/ipykernel_45947/1506314546.py:9: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have the most up-to-date information on Google's plans to challenge OpenAI. However, based on my knowledge, Google has been actively developing its own language model technology, called LaMDA (Language Model for Dialogue Applications). LaMDA is a conversational AI model that aims to engage in natural-sounding conversations with humans.\n",
      "\n",
      "In 2021, Google announced that LaMDA would be a key part of its plan to develop more advanced AI models. LaMDA has been shown to be capable of generating human-like responses to a wide range of questions and topics.\n",
      "\n",
      "Additionally, Google has been investing heavily in its AI research and development, with a focus on areas such as natural language processing, computer vision, and reinforcement learning. The company has also been making significant hires in the AI space, including the acquisition of companies like DeepMind, which developed the AlphaGo AI that defeated a human world champion in Go.\n",
      "\n",
      "While I don't have specific information on Google's plans to challenge OpenAI, it's likely that the company will continue to develop its own AI technology and compete with OpenAI in the market for conversational AI and language models.\n",
      "\n",
      "It's worth noting that Google has not publicly announced any specific plans to challenge OpenAI, and the company's AI strategy is likely to evolve over time as new technologies and opportunities emerge.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "groq_model=ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=groq_model, \n",
    "                                       chain_type= \"stuff\", \n",
    "                                       retriever=retriever)\n",
    "query = \"How Google plans to challenge OpenAI?\" \n",
    "response = qa_chain.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5bdc7aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(groq_model) \n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor = compressor,\n",
    "    base_retriever = retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eb1e1fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google opens up its AI language model PaLM to challenge OpenAI and GPT-3 Google offers developers access to one of its most advanced AI language models: PaLM. The search giant is launching an API for PaLM alongside a number of AI enterprise tools it says will help businesses \"generate text, images, code, videos, audio, and more from simple natural language prompts.\"\n",
      "PaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or Meta's LLaMA family of models.\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = compression_retriever.get_relevant_documents(\"How Google plans to challenge OpenAI?\") \n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "19379a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google plans to challenge OpenAI by offering developers access to one of its most advanced AI language models, PaLM, through an API. This will allow businesses to use PaLM to generate various content such as text, images, code, videos, and audio from simple natural language prompts. \n",
      "\n",
      "Google's move suggests that the company is positioning its AI enterprise tools, including the PaLM API, as a direct competitor to OpenAI's GPT-3 model and other similar offerings.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "groq_model=ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=groq_model, \n",
    "                                       chain_type= \"stuff\", \n",
    "                                       retriever=compression_retriever)\n",
    "query = \"How Google plans to challenge OpenAI?\" \n",
    "response = qa_chain.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f66d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6ad8a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8c0e39b",
   "metadata": {},
   "source": [
    "## Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c43988a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Manas Agarwal', 'age': 24}\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class Person(TypedDict):\n",
    "    name : str\n",
    "    age : int\n",
    "    \n",
    "new_person = Person(\n",
    "    name = \"Manas Agarwal\",\n",
    "    age = 24\n",
    ")\n",
    "print(new_person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb235d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Manas Agarwal', 'age': '24'}\n"
     ]
    }
   ],
   "source": [
    "new_person = Person(\n",
    "    name = \"Manas Agarwal\",\n",
    "    age = '24'\n",
    ")\n",
    "\n",
    "print(new_person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "758883d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cons': [\"Ayurvedic formula didn't work\",\n",
       "  'thick herbal texture',\n",
       "  'strong smell',\n",
       "  'dry and rough hair',\n",
       "  'scalp irritation',\n",
       "  'overpriced'],\n",
       " 'name': 'Manas Agarwal',\n",
       " 'pros': [],\n",
       " 'sentiment': 'neg',\n",
       " 'summary': \"Disappointed with the shampoo's performance and claims\",\n",
       " 'topic': ['hair fall',\n",
       "  'Ayurvedic formula',\n",
       "  'hair care',\n",
       "  'shampoo',\n",
       "  'product review']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict, Annotated, List, Optional, Literal\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "groq_model = ChatGroq(model = \"llama-3.1-8b-instant\",\n",
    "                      temperature = 0,\n",
    "                      max_retries = 2)\n",
    "\n",
    "## Schema\n",
    "\n",
    "class Review(TypedDict):\n",
    "    summary : Annotated[str,\"Summary review in 1 point\"]\n",
    "    sentiment : Annotated[Literal[\"pos\",\"neg\"],\"sentiment of review which is one of following == Positive -> pos or Negative -> neg\"]\n",
    "    topic : Annotated[List[str],\"Key Themes or list of topics talked about in review\"]\n",
    "    pros : Annotated[Optional[List[str]],\"List of pros of using the product\"]\n",
    "    cons : Annotated[Optional[List[str]],\"List of cons of using the product\"]\n",
    "    name : Annotated[Optional[str],\"Name of the person who reviewed the product\"]\n",
    "    \n",
    "structured_model = groq_model.with_structured_output(Review)\n",
    "\n",
    "result = structured_model.invoke(\"I had high hopes for Indulekha Bringha Shampoo after hearing all the hype about its Ayurvedic formula and promises to reduce hair fall. Unfortunately, it left me completely disappointed. After using it for a few weeks, I saw no improvement in my hair fall— if anything, it felt worse. The shampoo has a thick, herbal texture and a strong smell that’s not exactly pleasant, but I was willing to overlook that if it worked. It didn’t. My hair felt dry and rough after every wash, and the scalp irritation I started experiencing was the final straw. For a product that claims to be a “proprietary Ayurvedic medicine,” it didn’t deliver any noticeable benefits. It’s overpriced for what it is—just a basic cleanser that doesn’t live up to its claims. I won’t be repurchasing. Reviewed by Manas Agarwal\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fde8dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, EmailStr, Field\n",
    "from typing import Annotated, List, Optional, Dict\n",
    "\n",
    "class Student(BaseModel):\n",
    "    name: str = Field(max_length=50,description=\"Name of Student\")\n",
    "    CGPA : float = Field(gt=0,lt=10,description=\"Decimanl value indicating total Cumulative GPA over his graduation\")\n",
    "    age: Optional[int] = Field(gt=0, default=None, description=\"Age of student\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c15eb22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Review(summary='I had high hopes for Indulekha Bringha Shampoo after hearing all the hype about its Ayurvedic formula and promises to reduce hair fall. Unfortunately, it left me completely disappointed.', sentiment='neg', key_themes=['Ayurvedic formula', 'hair fall', 'hair texture', 'scalp irritation', 'price'], pros=None, cons=['thick herbal texture', 'strong smell', 'dry and rough hair', 'scalp irritation', 'overpriced'], name='null')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Annotated, List, Optional, Literal\n",
    "from pydantic import BaseModel,Field\n",
    "\n",
    "class Review(BaseModel):\n",
    "    summary : Annotated[str,Field(description=\"Summary review in 1 point\")]\n",
    "    sentiment : Annotated[Literal[\"pos\",\"neg\"],Field(description=\"sentiment of review which is one of following == Positive -> pos or Negative -> neg\")]\n",
    "    key_themes : Annotated[List[str],Field(description=\"Key Themes or list of topics talked about in review\")]\n",
    "    pros : Annotated[Optional[List[str]],Field(default=None,description=\"List of pros of using the product\")]\n",
    "    cons : Annotated[Optional[List[str]],Field(default=None,description = \"List of cons of using the product\")]\n",
    "    name : Annotated[Optional[str],Field(default= None, description =\"Name of the person who reviewed the product\")]\n",
    "    \n",
    "structured_model = groq_model.with_structured_output(Review)\n",
    "\n",
    "result = structured_model.invoke(\"I had high hopes for Indulekha Bringha Shampoo after hearing all the hype about its Ayurvedic formula and promises to reduce hair fall. Unfortunately, it left me completely disappointed. After using it for a few weeks, I saw no improvement in my hair fall— if anything, it felt worse. The shampoo has a thick, herbal texture and a strong smell that’s not exactly pleasant, but I was willing to overlook that if it worked. It didn’t. My hair felt dry and rough after every wash, and the scalp irritation I started experiencing was the final straw. For a product that claims to be a “proprietary Ayurvedic medicine,” it didn’t deliver any noticeable benefits. It’s overpriced for what it is—just a basic cleanser that doesn’t live up to its claims. I won’t be repurchasing.\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb9c0555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'I had high hopes for Indulekha Bringha Shampoo after hearing all the hype about its Ayurvedic formula and promises to reduce hair fall. Unfortunately, it left me completely disappointed.',\n",
       " 'sentiment': 'neg',\n",
       " 'key_themes': ['Ayurvedic formula',\n",
       "  'hair fall',\n",
       "  'hair texture',\n",
       "  'scalp irritation',\n",
       "  'price'],\n",
       " 'pros': None,\n",
       " 'cons': ['thick herbal texture',\n",
       "  'strong smell',\n",
       "  'dry and rough hair',\n",
       "  'scalp irritation',\n",
       "  'overpriced'],\n",
       " 'name': 'null'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f82496",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"title\" : 'Student',\n",
    "    \"description\" : \"Description about Student\",\n",
    "    \"type\" : \"object\",\n",
    "    \"properties\" : {\n",
    "        \"name\" : \"string\",\n",
    "        \"age\" : \"int\",\n",
    "    },\n",
    "    \"required\" : [\"name\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ae890e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Always Return a JSON object.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Give me a fictional person's name, age, city and profession\", additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "template = ChatPromptTemplate(\n",
    "    \n",
    "    messages= [(\"system\",\"Always {format_instruction}\"),\n",
    "               (\"human\",\"Give me a fictional person's name, age, city and profession\")],\n",
    "    \n",
    "    input_variables = [],\n",
    "    partial_variables = {\"format_instruction\":parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "prompt = template.invoke({})\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e5dd618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Always Return a JSON object.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"Give me a fictional person's name, age, city and profession\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9dbffb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Always Return a JSON object.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"Give me a fictional person's name, age, city and profession\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a51f848c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: Always Return a JSON object.\n",
      "Human: Give me a fictional person's name, age, city and profession\n"
     ]
    }
   ],
   "source": [
    "print(prompt.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be83b7f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Evelyn Stonebridge',\n",
       " 'age': 32,\n",
       " 'city': 'Los Angeles',\n",
       " 'profession': 'Software Engineer'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "groq_model = ChatGroq(model=\"llama-3.1-8b-instant\",temperature=1.5)\n",
    "chain = template | groq_model | parser\n",
    "\n",
    "chain.invoke({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c46d2840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Give me 3 facts about ultrasound during pregnancy\\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"fact_1\": string  // Fact 1 about the topic\\n\\t\"fact_2\": string  // Fact 2 about the topic\\n\\t\"fact_3\": string  // Fact 3 about the topic\\n}\\n```.')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "schema = [\n",
    "    ResponseSchema(name = \"fact_1\", description=\"Fact 1 about the topic\"),\n",
    "    ResponseSchema(name = \"fact_2\", description=\"Fact 2 about the topic\"),\n",
    "    ResponseSchema(name = \"fact_3\", description=\"Fact 3 about the topic\")\n",
    "]\n",
    "\n",
    "parser = StructuredOutputParser.from_response_schemas(schema)\n",
    "\n",
    "groq_model = ChatGroq(model=\"deepseek-r1-distill-llama-70b\")\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template= \"Give me 3 facts about {topic}\\n{format_instructions}.\",\n",
    "    input_variables = ['topic'],\n",
    "    partial_variables = {\"format_instructions\":parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "prompt_template.invoke({\"topic\":\"ultrasound during pregnancy\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "02b73380",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.invoke({\"topic\":\"ultrasound during pregnancy\"})\n",
    "result = groq_model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "82b9fd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so the user is asking for three facts about ultrasound during pregnancy. They also provided a specific format using markdown code. I need to make sure I follow their instructions precisely.\n",
      "\n",
      "First, I should recall what an ultrasound is. It's a prenatal test that uses high-frequency sound waves to create images of the baby inside the uterus. That's a good starting point for fact one.\n",
      "\n",
      "Next, I remember that ultrasounds are used for multiple purposes. They can confirm the pregnancy, check the heartbeat, determine the due date, and even identify the baby's position. That can be the second fact.\n",
      "\n",
      "For the third fact, I should mention that there are different types of ultrasounds. The standard 2D ultrasound is common, but there are also 3D and 4D options which provide more detailed images. This adds value as it shows the variety available.\n",
      "\n",
      "I need to structure these facts into a JSON schema within a markdown code snippet. Making sure each fact is concise and informative. Also, I have to include the leading and trailing triple backticks as they specified.\n",
      "\n",
      "I should double-check the accuracy of each fact. Ultrasound technology is non-invasive and uses sound waves, so it's safe. The purposes I mentioned are correct, and the types of ultrasounds are accurate as well.\n",
      "\n",
      "Finally, I'll format everything properly, ensuring the JSON structure is correct and the markdown is as they requested. This should meet the user's requirements effectively.\n",
      "</think>\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"fact_1\": \"Ultrasound during pregnancy uses high-frequency sound waves to create images of the developing fetus, allowing healthcare providers to monitor growth and development.\",\n",
      "\t\"fact_2\": \"The first trimester ultrasound can confirm pregnancy, detect the heartbeat, and estimate the due date, while later ultrasounds can check for fetal position and any potential abnormalities.\",\n",
      "\t\"fact_3\": \"There are different types of ultrasounds, including standard 2D, 3D, and 4D ultrasounds, with 3D and 4D providing more detailed and dynamic images of the fetus.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2f7d682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = parser.parse(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c2e22d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fact_1': 'Ultrasound during pregnancy uses high-frequency sound waves to create images of the developing fetus, allowing healthcare providers to monitor growth and development.',\n",
       " 'fact_2': 'The first trimester ultrasound can confirm pregnancy, detect the heartbeat, and estimate the due date, while later ultrasounds can check for fetal position and any potential abnormalities.',\n",
       " 'fact_3': 'There are different types of ultrasounds, including standard 2D, 3D, and 4D ultrasounds, with 3D and 4D providing more detailed and dynamic images of the fetus.'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9d35801b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Generate name, age and city of fictional person who belongs to India\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"description\": \"Name of the person\", \"title\": \"Name\", \"type\": \"string\"}, \"age\": {\"description\": \"Age of the person\", \"exclusiveMinimum\": 18, \"title\": \"Age\", \"type\": \"integer\"}, \"city\": {\"description\": \"This is the city where person lives\", \"title\": \"City\", \"type\": \"string\"}}, \"required\": [\"name\", \"age\", \"city\"]}\\n```')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "class Person(BaseModel):\n",
    "    name : str = Field(description=\"Name of the person\")\n",
    "    age : int = Field(description=\"Age of the person\",gt=18)\n",
    "    city : str = Field(description=\"This is the city where person lives\")\n",
    "    \n",
    "parser = PydanticOutputParser(pydantic_object=Person)\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template = \"Generate name, age and city of fictional person who belongs to {country}\\n{format_instructions}\",\n",
    "    input_variables= [\"country\"],\n",
    "    partial_variables={\"format_instructions\":parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "prompt = template.invoke({\"country\":\"India\"})\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "70708b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name='Rajesh Kumar', age=32, city='Mumbai')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groq_model = ChatGroq(model=\"deepseek-r1-distill-llama-70b\")\n",
    "\n",
    "chain = template | groq_model | parser\n",
    "chain.invoke({\"country\":\"India\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1dc40d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name='Emily Johnson', age=32, city='New York City')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"country\":\"USA\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1d1137b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name='Emily Carter', age=27, city='London')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"country\":\"England\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e638bac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
